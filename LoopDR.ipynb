{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.model_zoo import model_zoo\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "import ntpath\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torchvision\n",
    "from deepface import DeepFace\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an option:\n",
      "1. Upload video for detection\n",
      "2. Upload single image for detection and recognition\n",
      "3. Exit\n",
      "Invalid choice\n",
      "Select an option:\n",
      "1. Upload video for detection\n",
      "2. Upload single image for detection and recognition\n",
      "3. Exit\n",
      "Invalid choice\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import ntpath\n",
    "import datetime\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from deepface import DeepFace\n",
    "\n",
    "class Recognize:\n",
    "    def __init__(self):\n",
    "        self.cfg = get_cfg()\n",
    "        self.cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "        self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "        self.cfg.MODEL.DEVICE = 'cpu'  # Set device to GPU\n",
    "        self.predictor = DefaultPredictor(self.cfg)\n",
    "\n",
    "    def detect_faces(self, frame):\n",
    "        outputs = self.predictor(frame)\n",
    "        instances = outputs[\"instances\"]\n",
    "        pred_classes = instances.pred_classes if instances.has(\"pred_classes\") else None\n",
    "        boxes = instances.pred_boxes if instances.has(\"pred_boxes\") else None\n",
    "\n",
    "        detected_faces = []\n",
    "        if pred_classes is not None and boxes is not None:\n",
    "            for class_idx, box in zip(pred_classes, boxes):\n",
    "                if class_idx == 0:  # 0 corresponds to 'person' class in COCO dataset\n",
    "                    box = box.cpu().numpy().astype(int)  # Convert box tensor to numpy array\n",
    "                    x0, y0, x1, y1 = box[0], box[1], box[2], box[3]\n",
    "                    detected_faces.append(frame[y0:y1, x0:x1])\n",
    "        return detected_faces\n",
    "    \n",
    "    def format_timestamp(self, timestamp_ms):\n",
    "        utc_datetime = datetime.datetime.utcfromtimestamp(timestamp_ms / 1000.0)\n",
    "        timestamp = utc_datetime.strftime(\"%H%M%S\")\n",
    "        return timestamp\n",
    "\n",
    "    def save_faces(self, detected_faces, output_dir, video_name, timestamp_ms, frame_number):\n",
    "        video_name = os.path.splitext(video_name)[0]\n",
    "        timestamp = self.format_timestamp(timestamp_ms)\n",
    "\n",
    "        for i, face in enumerate(detected_faces):\n",
    "            face_name = f\"{video_name}_{timestamp}_{frame_number}_{i}.jpg\"\n",
    "            save_path = os.path.join(output_dir, face_name)\n",
    "            cv2.imwrite(save_path, face)\n",
    "            print(f\"Saved image: {face_name}\")\n",
    "        return len(detected_faces)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_frame_number(frame, frame_number):\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        position = (10, 30)\n",
    "        font_scale = 1\n",
    "        font_color = (0, 255, 0)  # Green color\n",
    "        line_type = 2\n",
    "\n",
    "        cv2.putText(frame, f\"Frame: {frame_number}\", position, font, font_scale, font_color, line_type)\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        output_dir = \"d_faces\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        frame_number = 0\n",
    "        total_detected_images = 0\n",
    "        frame_skip = 5  # Skip 5 frames after reading one frame\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "\n",
    "            if frame_number % 6 == 0:\n",
    "                detected_faces = self.detect_faces(frame)\n",
    "                count = self.save_faces(detected_faces, output_dir, ntpath.basename(video_path), timestamp_ms, frame_number)\n",
    "                total_detected_images += count\n",
    "\n",
    "            frame_number += 1\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number * frame_skip)\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        return total_detected_images\n",
    "\n",
    "    @staticmethod\n",
    "    def recognize_single_image(image_path):\n",
    "        # Placeholder for face recognition from single image\n",
    "        result = DeepFace.find(img_path=image_path, db_path=\"d_faces\", model_name=\"VGG-Face\")\n",
    "\n",
    "        print(\"Face recognition result:\")\n",
    "        print(result)\n",
    "\n",
    "    @staticmethod\n",
    "    def menu():\n",
    "        recognizer = Recognize()  # Define recognizer here\n",
    "        print(\"Select an option:\")\n",
    "        print(\"1. Upload video for detection\")\n",
    "        print(\"2. Upload single image for detection and recognition\")\n",
    "        print(\"3. Exit\")\n",
    "        choice = input(\"Enter your choice (1/2/3): \")\n",
    "\n",
    "        if choice == \"1\":\n",
    "            video_path = input(\"Enter the path to the video file: \")\n",
    "            recognizer.process_video(video_path)\n",
    "        elif choice == \"2\":\n",
    "            image_path = input(\"Enter the path to the image file: \")\n",
    "            recognizer.recognize_single_image(image_path)\n",
    "        elif choice == \"3\":\n",
    "            print(\"Exiting...\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Invalid choice\")\n",
    "\n",
    "        # Ask for next action\n",
    "        Recognize.menu()\n",
    "\n",
    "# Example usage:\n",
    "Recognize.menu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
